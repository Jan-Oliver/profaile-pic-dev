{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/diffusers.git\n",
    "!cd /content/diffusers && pip install .\n",
    "!pip install transformers scipy ftfy accelerate\n",
    "!pip install \"ipywidgets>=7,<8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Generation\n",
    "\n",
    "\n",
    "Allows to tweak your prompts on a specific result you liked by generating own latents.\n",
    "\n",
    "In order to reuse the seeds we need to generate the latents ourselves. Otherwise, the pipeline will do it internally and we won't have a way to replicate them.\n",
    "Latents are the initial random Gaussian noise that gets transformed to actual images during the diffusion process.\n",
    "To generate them, we'll use a different random seed for each latent, and we'll save them so we can reuse them later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb#scrollTo=cf996058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"scheduler\")\n",
    "\n",
    "# revision and dtype make sure that we use lower GPU memory but maybe sacrifice some of the quality\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1-base\",\n",
    "    scheduler=scheduler, \n",
    "    revision=\"fp16\", \n",
    "    torch_dtype=torch.float16,\n",
    "    requires_safety_checker=False\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 4\n",
    "\n",
    "width = 512\n",
    "height = 512\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\")\n",
    "\n",
    "latents = None\n",
    "seeds = []\n",
    "for _ in range(num_images):\n",
    "    # Get a new random seed, store it and use it as the generator state\n",
    "    seed = generator.seed()\n",
    "    seeds.append(seed)\n",
    "    generator = generator.manual_seed(seed)\n",
    "    \n",
    "    image_latents = torch.randn(\n",
    "        (1, pipe.unet.in_channels, height // 8, width // 8),\n",
    "        generator = generator,\n",
    "        device = \"cuda\"\n",
    "    )\n",
    "    latents = image_latents if latents is None else torch.cat((latents, image_latents))\n",
    "    \n",
    "# latents should have shape (4, 4, 64, 64) in this case\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now send latents to pipe\n",
    "prompt = \"Labrador in the style of Vermeer\"\n",
    "\n",
    "with torch.autocast(\"cuda\"):\n",
    "    images = pipe(\n",
    "        [prompt] * num_images,\n",
    "        guidance_scale=7.5,\n",
    "        latents = latents,\n",
    "    ).images\n",
    "\n",
    "image_grid(images, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to have more of style 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = seeds[2]   # Third\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate latents given seed\n",
    "generator.manual_seed(seed)\n",
    "\n",
    "latents = torch.randn(\n",
    "    (1, pipe.unet.in_channels, height // 8, width // 8),\n",
    "    generator = generator,\n",
    "    device = \"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(\"cuda\"):\n",
    "    image = pipe(\n",
    "        [prompt] * 1,\n",
    "        guidance_scale=7.5,\n",
    "        latents = latents,\n",
    "    ).images\n",
    "    \n",
    "image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Clown in the style of Vermeer\"\n",
    "\n",
    "with torch.autocast(\"cuda\"):\n",
    "    image = pipe(\n",
    "        [prompt] * 1,\n",
    "        guidance_scale=7.5,\n",
    "        latents = latents,\n",
    "    ).images\n",
    "    \n",
    "image[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
